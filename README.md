# machine learning 

## Introduction：
This project is the implementation of the algorithems based on the courses `Machine Learning Foundation` and `Machine Learning Technique` from `NTU`.

## Contents：
  * `AdaBoost.py:` adaptive boosting classifier with the base learner decision stump.

  * `DecisionStump.py:` decision stump supporting weighted data.

  * `DecisionTree.py:` an implementation of the classification and regression tree(CART) algorithem.

  * `GBDTRegressor.py:` a gradient boosting regressor using pruned decision tree, with the mean square error as loss function.

  * `KNN.py:` the k-nearest neighbour classifier and regressor.

  * `Kmeans.py:` K-means clustering.

  * `LinearRegressor.py:` linear regression w/ and w/o interception, supporting L2-regularized(ridge) regression and kernel ridge regression(linear, polynomial and gaussion)

  * `LogisticRegressor.py:` logistic regression, L2-regularized LR based on stochastic gradient descent(SGD).

  * `MatrixFactorization.py:` matrix factorization, supporting alternative least square(ALS) and SGD.

  * `NaiveBayes.py:` naive bayes classifier, supporting multinomial and gaussion.

  * `NeuralNetwork.py:` an implementation of the neural network and back propagation(BP), training with mini-batch.

  * `NonSeparablePerceptron.py:` non-seperable perceptron, training with pocket algorithem.

  * `PCA.py:` Principle Component Analysis with SVD and Linear Discriminant Analysis(LDA).

  * `Perceptron.py:` an implementation of peceptron with PLA algorithem.

  * `RandomForest.py:` a simple implementation of random forest classifier, supporting feature selection with the OOB error.

  * `SVM.py:` soft margin SVM, probabilistic SVM and Support Vector Regression(SVR), supporting linear, polynomial and gaussion kernels.

